{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1ly55CM0TSCTIUM81kQzyXq0WsIdTgmOI","authorship_tag":"ABX9TyM3nS1vBIeZxyIJBPPjAvxI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install streamlit"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ojzrnE73Q3nT","executionInfo":{"status":"ok","timestamp":1753676502977,"user_tz":-330,"elapsed":11821,"user":{"displayName":"Gowtham Anbazhagan","userId":"09398642683718980526"}},"outputId":"b7ec8cb5-eb09-419b-e59b-2c9a8ce6adff"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting streamlit\n","  Downloading streamlit-1.47.1-py3-none-any.whl.metadata (9.0 kB)\n","Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n","Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n","Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n","Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n","Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n","Collecting watchdog<7,>=2.1.5 (from streamlit)\n","  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n","Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n","  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n","Downloading streamlit-1.47.1-py3-none-any.whl (9.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n","Successfully installed pydeck-0.9.1 streamlit-1.47.1 watchdog-6.0.0\n"]}]},{"cell_type":"code","source":["!pip install pyngrok"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UX7ZqTSpRDpg","executionInfo":{"status":"ok","timestamp":1753676555237,"user_tz":-330,"elapsed":6926,"user":{"displayName":"Gowtham Anbazhagan","userId":"09398642683718980526"}},"outputId":"eb294250-45c7-4d9e-c0af-7b6c9307a362"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyngrok\n","  Downloading pyngrok-7.2.12-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n","Downloading pyngrok-7.2.12-py3-none-any.whl (26 kB)\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-7.2.12\n"]}]},{"cell_type":"code","source":["%%writefile app.py\n","import streamlit as st\n","import pandas as pd\n","import pickle\n","import plotly.express as px\n","from sklearn.preprocessing import StandardScaler\n","\n","st.set_page_config(page_title=\"Customer Conversion Analysis\", layout=\"wide\")\n","\n","@st.cache_resource\n","def load_pickle(file_path):\n","    with open(file_path, 'rb') as file:\n","        return pickle.load(file)\n","\n","class_model = load_pickle(\"/content/drive/MyDrive/ClickStream_Project/Pickles/best_model_class.pkl\")\n","reg_model = load_pickle(\"/content/drive/MyDrive/ClickStream_Project/Pickles/best_model_reg.pkl\")\n","clust_model = load_pickle(\"/content/drive/MyDrive/ClickStream_Project/Pickles/best_model_clust.pkl\")\n","\n","try:\n","    preprocessor = load_pickle(\"/content/drive/MyDrive/ClickStream_Project/Pickles/preprocessed_data.pkl\")\n","except Exception as e:\n","    st.warning(f\"Could not load preprocessor: {e}\")\n","    preprocessor = None\n","\n","def debug_model_features(model, df):\n","\n","    st.write(\"Model Debugging Information\")\n","\n","    if isinstance(model, dict):\n","        st.write(\"Model is a dictionary with the following keys:\")\n","        st.write(list(model.keys()))\n","\n","        for key, value in model.items():\n","            if hasattr(value, 'feature_names_in_'):\n","                st.write(f\"Found model with feature information in key '{key}'\")\n","                st.write(f\"This model expects {len(value.feature_names_in_)} features:\")\n","                st.write(value.feature_names_in_)\n","\n","                common_features = [col for col in value.feature_names_in_ if col in df.columns]\n","                st.write(f\"Found {len(common_features)}/{len(value.feature_names_in_)} expected features in data\")\n","\n","                if len(common_features) < len(value.feature_names_in_):\n","                    missing = [f for f in value.feature_names_in_ if f not in df.columns]\n","                    st.write(\"Missing features:\")\n","                    st.write(missing)\n","                return\n","            elif hasattr(value, 'n_features_in_'):\n","                st.write(f\"Model in key '{key}' expects {value.n_features_in_} features but doesn't provide feature names\")\n","                return\n","\n","        st.write(\"No sklearn model with feature information found in the dictionary\")\n","        st.write(\"Available dataframe features:\")\n","        st.write(df.columns.tolist())\n","\n","    elif hasattr(model, 'feature_names_in_'):\n","        st.write(f\"Model expects {len(model.feature_names_in_)} features:\")\n","        st.write(model.feature_names_in_)\n","\n","        common_features = [col for col in model.feature_names_in_ if col in df.columns]\n","        st.write(f\"Found {len(common_features)}/{len(model.feature_names_in_)} expected features in data\")\n","\n","        if len(common_features) < len(model.feature_names_in_):\n","            missing = [f for f in model.feature_names_in_ if f not in df.columns]\n","            st.write(\"Missing features:\")\n","            st.write(missing)\n","\n","    elif hasattr(model, 'n_features_in_'):\n","        st.write(f\"Model expects {model.n_features_in_} features but doesn't provide feature names\")\n","        st.write(\"Available dataframe features:\")\n","        st.write(df.columns.tolist())\n","\n","    else:\n","        st.write(\"Model doesn't provide feature information\")\n","        st.write(\"Available dataframe features:\")\n","        st.write(df.columns.tolist())\n","\n","st.sidebar.title(\"Pages\")\n","\n","page = st.sidebar.radio(\"Go to\", [\"Upload Data\", \"Manual Entry\", \"Classification\", \"Regression\", \"Clustering\"])\n","\n","if \"df_features\" not in st.session_state:\n","    st.session_state.df_features = None\n","\n","if page == \"Upload Data\":\n","\n","    st.title(\"Upload CSV File\")\n","\n","    uploaded_file = st.file_uploader(\"Upload your dataset (CSV format)\", type=[\"csv\"])\n","    if uploaded_file is not None:\n","        data = pd.read_csv(uploaded_file)\n","        st.write(\"Data Preview:\")\n","        st.write(data.head())\n","\n","        if st.button('Compute New Features'):\n","            df_features = data.copy()\n","            total_clicks = df_features.groupby('session_id')['order'].count()\n","            avg_price = df_features.groupby('session_id')['price'].mean()\n","            unique_products = df_features.groupby('session_id')['page2_clothing_model'].nunique()\n","            browsing_depth = df_features.groupby('session_id')['page'].max()\n","            df_features['total_clicks'] = df_features['session_id'].map(total_clicks)\n","            df_features['avg_price'] = df_features['session_id'].map(avg_price)\n","            df_features['unique_products'] = df_features['session_id'].map(unique_products)\n","            df_features['browsing_depth'] = df_features['session_id'].map(browsing_depth)\n","            df_features['date'] = pd.to_datetime(df_features[['year', 'month', 'day']].astype(str).agg('-'.join, axis=1))\n","            df_features['weekday'] = df_features['date'].dt.dayofweek\n","            df_features['weekend'] = (df_features['weekday'] >= 5).astype(int)\n","            median_price = df_features['price'].median()\n","            df_features['high_price_preference'] = (df_features['price'] > median_price).astype(int)\n","            df_features.drop(columns=['date'], inplace = True)\n","\n","            st.session_state.df_features = df_features\n","\n","            st.write(\"Feature Engineered Data\")\n","            st.write(df_features.head())\n","\n","        if st.button(\"Save Data\"):\n","            if st.session_state.df_features is not None:\n","                st.write(\"Data saved successfully for further analysis.\")\n","            else:\n","                st.warning(\"Please compute features before saving.\")\n","\n","\n","elif page == \"Manual Entry\":\n","\n","    st.title(\"Enter Data Manually for Analysis\")\n","\n","    country_mappings = {\n","        1: 'Australia', 2: 'Austria', 3: 'Belgium', 4: 'British Virgin Islands', 5: 'Cayman Islands',\n","        6: 'Christmas Island', 7: 'Croatia', 8: 'Cyprus', 9: 'Czech Republic', 10: 'Denmark',\n","        11: 'Estonia', 12: 'unidentified', 13: 'Faroe Islands', 14: 'Finland', 15: 'France',\n","        16: 'Germany', 17: 'Greece', 18: 'Hungary', 19: 'Iceland', 20: 'India', 21: 'Ireland',\n","        22: 'Italy', 23: 'Latvia', 24: 'Lithuania', 25: 'Luxembourg', 26: 'Mexico', 27: 'Netherlands',\n","        28: 'Norway', 29: 'Poland', 30: 'Portugal', 31: 'Romania', 32: 'Russia', 33: 'San Marino',\n","        34: 'Slovakia', 35: 'Slovenia', 36: 'Spain', 37: 'Sweden', 38: 'Switzerland', 39: 'Ukraine',\n","        40: 'United Arab Emirates', 41: 'United Kingdom', 42: 'USA', 43: 'biz (.biz)', 44: 'com (.com)',\n","        45: 'int (.int)', 46: 'net (.net)', 47: 'org (*.org)'\n","    }\n","\n","    category_mappings = {\n","        1: 'Trousers', 2: 'Skirts', 3: 'Blouses', 4: 'Sales'\n","    }\n","\n","    color_mappings = {\n","        1: 'beige',2: 'black',3: 'blue',4: 'brown',5: 'burgundy',6: 'gray',7: 'green',8: 'navy blue',9: 'many colors',\n","        10: 'olive',11: 'pink',12: 'red',13: 'violet',14: 'white'\n","    }\n","\n","    location_mappings = {\n","    1:'top left',2:'top in the middle',3:'top right',4:'bottom left',5:'bottom in the middle',6:'bottom right'\n","    }\n","    model_mappings = {\n","        1:'Face', 2:'Profile'\n","    }\n","\n","    page_mappings = {\n","        1: \"Home\", 2: \"Category\", 3: \"Product\", 4: \"Cart\", 5: \"Checkout\"\n","    }\n","\n","    year = st.selectbox(\"Select Year\", [2008])\n","    month = st.slider(\"Enter Month\", 1, 12, 1)\n","    day = st.slider(\"Enter Day\", 1, 31, 1)\n","    session_id = st.number_input(\"Select Session ID\", min_value = 1000, max_value = 99999)\n","    order = st.slider(\"Enter Click Sequence\", 1, 200, 1)\n","    country = st.selectbox(\"Select Country\", list(country_mappings.values()))\n","    category = st.selectbox(\"Select Category\", list(category_mappings.values()))\n","    model_number = st.text_area(\"Enter Model Number\")\n","    color = st.selectbox(\"Select Colour\", list(color_mappings.values()))\n","    location = st.selectbox(\"Select Photo Location\", list(location_mappings.values()))\n","    model = st.selectbox(\"Select Model Photo\", list(model_mappings.values()))\n","    price = st.slider(\"Enter Price ($)\", 10, 200, 10)\n","    page = st.selectbox(\"Select Page\", list(page_mappings.values()))\n","\n","    country = list(country_mappings.keys())[list(country_mappings.values()).index(country)]\n","    category = list(category_mappings.keys())[list(category_mappings.values()).index(category)]\n","    color = list(color_mappings.keys())[list(color_mappings.values()).index(color)]\n","    location = list(location_mappings.keys())[list(location_mappings.values()).index(location)]\n","    model = list(model_mappings.keys())[list(model_mappings.values()).index(model)]\n","    page = list(page_mappings.keys())[list(page_mappings.values()).index(page)]\n","\n","    user_input = pd.DataFrame([{\n","        \"year\": year, \"month\": month, \"day\": day, \"session_id\": session_id, \"order\": order, \"country\": country, \"page1_main_category\": category,\n","        \"page2_clothing_model\":model_number , \"colour\": color, \"location\": location, \"model_photography\": model, \"price\": price, \"page\": page\n","    }])\n","\n","    if st.button(\"View Entered Data\"):\n","        st.write(\"Entered Data\")\n","        st.write(user_input)\n","\n","    if st.button(\"Compute New Features\"):\n","        df_features = user_input.copy()\n","        total_clicks = df_features.groupby('session_id')['order'].count()\n","        avg_price = df_features.groupby('session_id')['price'].mean()\n","        unique_products = df_features.groupby('session_id')['page2_clothing_model'].nunique()\n","        browsing_depth = df_features.groupby('session_id')['page'].max()\n","        df_features['total_clicks'] = df_features['session_id'].map(total_clicks)\n","        df_features['avg_price'] = df_features['session_id'].map(avg_price)\n","        df_features['unique_products'] = df_features['session_id'].map(unique_products)\n","        df_features['browsing_depth'] = df_features['session_id'].map(browsing_depth)\n","        df_features['date'] = pd.to_datetime(df_features[['year', 'month', 'day']].astype(str).agg('-'.join, axis=1))\n","        df_features['weekday'] = df_features['date'].dt.dayofweek\n","        df_features['weekend'] = (df_features['weekday'] >= 5).astype(int)\n","        median_price = 100\n","        df_features['high_price_preference'] = (df_features['price'] > median_price).astype(int)\n","        df_features.drop(columns=['date'], inplace = True)\n","        st.session_state.df_features = df_features\n","        st.write(\"Feature Engineered Data\")\n","        st.write(df_features)\n","\n","    if st.button(\"Save Data\"):\n","        if st.session_state.df_features is not None:\n","            st.write(\"Data saved successfully for further analysis.\")\n","        else:\n","            st.warning(\"Please compute features before saving.\")\n","\n","elif page == \"Classification\":\n","\n","    st.title(\"Customer Purchase Prediction\")\n","    st.write(\"Predict if a customer will complete a purchase.\")\n","\n","    if st.session_state.df_features is not None:\n","        st.write(\"### Feature Engineered Data Used for Classification\")\n","        st.write(st.session_state.df_features.head())\n","\n","        debug_model_features(class_model, st.session_state.df_features)\n","\n","        try:\n","            if hasattr(class_model, 'feature_names_in_'):\n","                common_features = [col for col in class_model.feature_names_in_\n","                                  if col in st.session_state.df_features.columns]\n","\n","                if len(common_features) == len(class_model.feature_names_in_):\n","                    st.write(\"Using exact features expected by model...\")\n","                    prediction_data = st.session_state.df_features[common_features]\n","                    predictions = class_model.predict(prediction_data)\n","                    st.session_state.df_features[\"Prediction\"] = predictions\n","\n","                    st.write(\"Classification Results\")\n","                    display_cols = ['page1_main_category', 'colour', 'location',\n","                                  'model_photography', 'page', 'high_price_preference', 'Prediction']\n","                    st.write(st.session_state.df_features[display_cols])\n","\n","                else:\n","                    st.warning(f\"Only found {len(common_features)} of {len(class_model.feature_names_in_)} required features !!!\")\n","                    st.write(\"Try using an alternative approach...\")\n","                    raise ValueError(\"Insufficient features !!!\")\n","            else:\n","                raise ValueError(\"Model doesn't provide feature information\")\n","\n","        except Exception as e:\n","            st.write(f\"Direct prediction failed: {str(e)}\")\n","\n","            st.write(\"Trying alternative approach with basic features...\")\n","\n","            basic_features = ['total_clicks', 'avg_price', 'unique_products',\n","                            'browsing_depth', 'page', 'price', 'high_price_preference',\n","                            'weekday', 'weekend']\n","\n","            available_features = [col for col in basic_features if col in st.session_state.df_features.columns]\n","\n","            if len(available_features) > 0:\n","                st.write(f\"Using {len(available_features)} basic features for prediction\")\n","\n","                df_for_prediction = st.session_state.df_features[available_features]\n","\n","                scaler = StandardScaler()\n","                numeric_features = df_for_prediction.select_dtypes(include=['float64', 'int64']).columns\n","                df_for_prediction[numeric_features] = scaler.fit_transform(df_for_prediction[numeric_features])\n","\n","                try:\n","                    predictions = class_model.predict(df_for_prediction)\n","                    st.session_state.df_features[\"Prediction\"] = predictions\n","\n","                    st.write(\"Classification Results (using basic features)\")\n","\n","                    display_cols = ['page1_main_category', 'colour', 'location',\n","                                  'model_photography', 'page', 'high_price_preference', 'Prediction']\n","                    st.write(st.session_state.df_features[display_cols])\n","\n","                except Exception as e:\n","                    st.error(f\"Alternative prediction failed: {str(e)}\")\n","                    st.write(\"The model and current data features are incompatible.\")\n","                    st.write(\"Consider retraining your model with these features or using a compatible dataset.\")\n","            else:\n","                st.error(\"Cannot find compatible features for this model\")\n","    else:\n","        st.warning(\"Please upload or manually enter data before running classification.\")\n","\n","elif page == \"Regression\":\n","\n","    st.title(\"Customer Spending Prediction\")\n","    st.write(\"Predict how much a customer will spend.\")\n","\n","    if st.session_state.df_features is not None:\n","        st.write(\"Feature Engineered Data Used for Regression\")\n","        st.write(st.session_state.df_features.head())\n","\n","        debug_model_features(reg_model, st.session_state.df_features)\n","\n","        try:\n","            if hasattr(reg_model, 'feature_names_in_'):\n","                common_features = [col for col in reg_model.feature_names_in_\n","                                  if col in st.session_state.df_features.columns]\n","\n","                if len(common_features) == len(reg_model.feature_names_in_):\n","                    st.write(\"Using exact features expected by model\")\n","                    prediction_data = st.session_state.df_features[common_features]\n","                    predictions = reg_model.predict(prediction_data)\n","                    st.session_state.df_features[\"Predicted_Spending\"] = predictions\n","\n","                    st.write(\"Regression Results\")\n","                    display_cols = ['page1_main_category', 'colour', 'location',\n","                                  'model_photography', 'page', 'high_price_preference', 'Predicted_Spending']\n","                    st.write(st.session_state.df_features[display_cols])\n","\n","                    st.write(\"Visualization of Predictions\")\n","                    if 'total_clicks' in st.session_state.df_features.columns:\n","                        fig = px.scatter(st.session_state.df_features,\n","                                        x='total_clicks',\n","                                        y='Predicted_Spending',\n","                                        color='high_price_preference' if 'high_price_preference' in st.session_state.df_features.columns else None,\n","                                        hover_data=['page1_main_category'] if 'page1_main_category' in st.session_state.df_features.columns else None)\n","                        st.plotly_chart(fig)\n","                else:\n","                    st.warning(f\"Only found {len(common_features)} of {len(reg_model.feature_names_in_)} required features\")\n","                    st.write(\"Try using an alternative approach...\")\n","                    raise ValueError(\"Insufficient features\")\n","            else:\n","                raise ValueError(\"Model doesn't provide feature information\")\n","\n","        except Exception as e:\n","            st.write(f\"Direct prediction failed: {str(e)}\")\n","\n","            st.write(\"Trying alternative approach with basic features...\")\n","\n","            basic_features = ['total_clicks', 'avg_price', 'unique_products',\n","                            'browsing_depth', 'page', 'price', 'high_price_preference',\n","                            'weekday', 'weekend']\n","\n","            available_features = [col for col in basic_features if col in st.session_state.df_features.columns]\n","\n","            if len(available_features) > 0:\n","                st.write(f\"Using {len(available_features)} basic features for prediction\")\n","\n","                df_for_prediction = st.session_state.df_features[available_features]\n","\n","                scaler = StandardScaler()\n","                numeric_features = df_for_prediction.select_dtypes(include=['float64', 'int64']).columns\n","                df_for_prediction[numeric_features] = scaler.fit_transform(df_for_prediction[numeric_features])\n","\n","                try:\n","                    predictions = reg_model.predict(df_for_prediction)\n","                    st.session_state.df_features[\"Predicted_Spending\"] = predictions\n","\n","                    st.write(\"Regression Results (using basic features)\")\n","                    display_cols = ['page1_main_category', 'colour', 'location',\n","                                  'model_photography', 'page', 'high_price_preference', 'Predicted_Spending']\n","                    st.write(st.session_state.df_features[display_cols])\n","\n","                    st.write(\"Visualization of Predictions\")\n","                    fig = px.histogram(st.session_state.df_features, x='Predicted_Spending',\n","                                      nbins=20, title='Distribution of Predicted Spending')\n","                    st.plotly_chart(fig)\n","\n","                except Exception as e:\n","                    st.error(f\"Alternative prediction failed: {str(e)}\")\n","                    st.write(\"The model and current data features are incompatible.\")\n","                    st.write(\"Consider retraining your model with these features or using a compatible dataset.\")\n","            else:\n","                st.error(\"Cannot find compatible features for this model\")\n","    else:\n","        st.warning(\"Please upload or manually enter data before running regression analysis.\")\n","\n","elif page == \"Clustering\":\n","\n","    st.title(\"Customer Segmentation\")\n","    st.write(\"Group customers into segments based on their behavior.\")\n","\n","    if st.session_state.df_features is not None:\n","        st.write(\"Feature Engineered Data Used for Clustering\")\n","        st.write(st.session_state.df_features.head())\n","\n","        debug_model_features(clust_model, st.session_state.df_features)\n","\n","        try:\n","\n","            if isinstance(clust_model, dict):\n","\n","                if 'kmeans' in clust_model and hasattr(clust_model['kmeans'], 'predict'):\n","                    actual_model = clust_model['kmeans']\n","                    st.write(\"Using KMeans model from dictionary\")\n","\n","                else:\n","                    model_found = False\n","                    for key, value in clust_model.items():\n","                        if hasattr(value, 'predict') or hasattr(value, 'fit_predict'):\n","                            actual_model = value\n","                            model_found = True\n","                            st.write(f\"Using model from dictionary key: {key}\")\n","                            break\n","\n","                    if not model_found:\n","                        st.write(\"No usable model found in dictionary. Creating a new KMeans model.\")\n","                        from sklearn.cluster import KMeans\n","\n","                        basic_features = ['total_clicks', 'avg_price', 'unique_products',\n","                                        'browsing_depth', 'price', 'high_price_preference']\n","                        available_features = [col for col in basic_features if col in st.session_state.df_features.columns]\n","\n","                        if len(available_features) > 0:\n","\n","                            df_for_clustering = st.session_state.df_features[available_features]\n","\n","                            scaler = StandardScaler()\n","                            numeric_features = df_for_clustering.select_dtypes(include=['float64', 'int64']).columns\n","                            df_for_clustering[numeric_features] = scaler.fit_transform(df_for_clustering[numeric_features])\n","\n","                            actual_model = KMeans(n_clusters=4, random_state=42)\n","                            actual_model.fit(df_for_clustering)\n","\n","                            common_features = available_features\n","                            clustering_data = df_for_clustering\n","                            st.write(\"Using newly created KMeans model with basic features\")\n","\n","                            clusters = actual_model.predict(clustering_data)\n","                            st.session_state.df_features[\"Cluster\"] = clusters\n","\n","                            st.write(\"On-the-fly Clustering Results\")\n","                            display_cols = ['page1_main_category', 'colour', 'location',\n","                                          'model_photography', 'page', 'high_price_preference', 'Cluster']\n","                            st.write(st.session_state.df_features[display_cols])\n","\n","                            st.write(\"Cluster Centers (standardized features)\")\n","                            cluster_centers_df = pd.DataFrame(\n","                                actual_model.cluster_centers_,\n","                                columns=available_features\n","                            )\n","                            st.write(cluster_centers_df)\n","\n","                            st.write(\"Cluster Visualization\")\n","                            cluster_counts = st.session_state.df_features[\"Cluster\"].value_counts().reset_index()\n","                            cluster_counts.columns = [\"Cluster\", \"Count\"]\n","                            fig = px.pie(cluster_counts, names=\"Cluster\", values=\"Count\",\n","                                       title=\"Customer Segment Distribution\")\n","                            st.plotly_chart(fig)\n","\n","                            st.write(\"Cluster Analysis\")\n","                            numeric_cols = st.session_state.df_features.select_dtypes(include=['float64', 'int64']).columns\n","                            cluster_profiles = st.session_state.df_features.groupby('Cluster')[numeric_cols].mean()\n","                            st.write(cluster_profiles)\n","\n","                            continue_regular_flow = False\n","                        else:\n","                            st.error(\"Cannot find any basic features for clustering\")\n","                            continue_regular_flow = False\n","                    else:\n","                        continue_regular_flow = True\n","\n","            else:\n","                actual_model = clust_model\n","                continue_regular_flow = True\n","\n","            if hasattr(actual_model, 'feature_names_in_'):\n","                common_features = [col for col in actual_model.feature_names_in_\n","                                  if col in st.session_state.df_features.columns]\n","\n","                if len(common_features) == len(actual_model.feature_names_in_):\n","                    st.write(\"Using exact features expected by model\")\n","                    clustering_data = st.session_state.df_features[common_features]\n","                    clusters = actual_model.predict(clustering_data)\n","                    st.session_state.df_features[\"Cluster\"] = clusters\n","\n","                    st.write(\"Clustering Results\")\n","                    display_cols = ['page1_main_category', 'colour', 'location',\n","                                  'model_photography', 'page', 'high_price_preference', 'Cluster']\n","                    st.write(st.session_state.df_features[display_cols])\n","\n","                    st.write(\"Cluster Analysis\")\n","                    cluster_counts = st.session_state.df_features[\"Cluster\"].value_counts().reset_index()\n","                    cluster_counts.columns = [\"Cluster\", \"Count\"]\n","\n","                    fig1 = px.bar(cluster_counts, x=\"Cluster\", y=\"Count\",\n","                                 title=\"Number of Customers in Each Cluster\")\n","                    st.plotly_chart(fig1)\n","\n","                    if 'total_clicks' in st.session_state.df_features.columns and 'avg_price' in st.session_state.df_features.columns:\n","                        fig2 = px.scatter(st.session_state.df_features,\n","                                        x='total_clicks',\n","                                        y='avg_price',\n","                                        color='Cluster',\n","                                        hover_data=['page1_main_category'] if 'page1_main_category' in st.session_state.df_features.columns else None,\n","                                        title=\"Cluster Distribution by Total Clicks and Average Price\")\n","                        st.plotly_chart(fig2)\n","\n","                    st.write(\"Cluster Characteristics\")\n","                    numeric_cols = st.session_state.df_features.select_dtypes(include=['float64', 'int64']).columns\n","                    cluster_profiles = st.session_state.df_features.groupby('Cluster')[numeric_cols].mean()\n","                    st.write(cluster_profiles)\n","\n","                else:\n","                    st.warning(f\"Only found {len(common_features)} of {len(actual_model.feature_names_in_)} required features\")\n","                    st.write(\"Try using an alternative approach...\")\n","                    raise ValueError(\"Insufficient features\")\n","            else:\n","                raise ValueError(\"Model doesn't provide feature information\")\n","\n","        except Exception as e:\n","            st.write(f\"Direct clustering failed: {str(e)}\")\n","\n","            st.write(\"Trying alternative approach with basic features...\")\n","\n","            basic_features = ['total_clicks', 'avg_price', 'unique_products',\n","                            'browsing_depth', 'price', 'high_price_preference']\n","\n","            available_features = [col for col in basic_features if col in st.session_state.df_features.columns]\n","\n","            if len(available_features) > 0:\n","                st.write(f\"Using {len(available_features)} basic features for clustering\")\n","\n","                df_for_clustering = st.session_state.df_features[available_features]\n","\n","                scaler = StandardScaler()\n","                numeric_features = df_for_clustering.select_dtypes(include=['float64', 'int64']).columns\n","                df_for_clustering[numeric_features] = scaler.fit_transform(df_for_clustering[numeric_features])\n","\n","                try:\n","\n","                    if isinstance(clust_model, dict):\n","                        model_found = False\n","                        if 'kmeans' in clust_model and hasattr(clust_model['kmeans'], 'predict'):\n","                            clusters = clust_model['kmeans'].predict(df_for_clustering)\n","                            model_found = True\n","                            st.write(\"Using existing KMeans model from dictionary\")\n","                        else:\n","\n","                            for key, value in clust_model.items():\n","                                if hasattr(value, 'predict'):\n","                                    clusters = value.predict(df_for_clustering)\n","                                    model_found = True\n","                                    st.write(f\"Using model from key '{key}' for prediction\")\n","                                    break\n","\n","                        if not model_found:\n","                            st.write(\"No usable clustering model found. Creating a new KMeans model on the fly.\")\n","                            from sklearn.cluster import KMeans\n","\n","                            temp_model = KMeans(n_clusters=4, random_state=42)\n","                            temp_model.fit(df_for_clustering)\n","                            clusters = temp_model.labels_\n","\n","                            st.write(\"On-the-fly KMeans Clustering Results\")\n","                            st.write(\"Cluster centers (standardized features):\")\n","                            cluster_centers_df = pd.DataFrame(\n","                                temp_model.cluster_centers_,\n","                                columns=df_for_clustering.columns\n","                            )\n","                            st.write(cluster_centers_df)\n","                    else:\n","                        if hasattr(clust_model, 'predict'):\n","                            clusters = clust_model.predict(df_for_clustering)\n","                        else:\n","                            st.write(\"Model doesn't have predict method. Creating a new KMeans model.\")\n","                            from sklearn.cluster import KMeans\n","                            temp_model = KMeans(n_clusters=4, random_state=42)\n","                            temp_model.fit(df_for_clustering)\n","                            clusters = temp_model.labels_\n","                    st.session_state.df_features[\"Cluster\"] = clusters\n","\n","                    st.write(\"Clustering Results (using basic features)\")\n","                    display_cols = ['page1_main_category', 'colour', 'location',\n","                                  'model_photography', 'page', 'high_price_preference', 'Cluster']\n","                    st.write(st.session_state.df_features[display_cols])\n","\n","                    cluster_counts = st.session_state.df_features[\"Cluster\"].value_counts().reset_index()\n","                    cluster_counts.columns = [\"Cluster\", \"Count\"]\n","                    fig = px.pie(cluster_counts, names=\"Cluster\", values=\"Count\",\n","                               title=\"Customer Segment Distribution\")\n","                    st.plotly_chart(fig)\n","\n","                    st.write(\"Basic Cluster Characteristics\")\n","                    cluster_means = st.session_state.df_features.groupby('Cluster')[available_features].mean()\n","                    st.write(cluster_means)\n","\n","                except Exception as e:\n","                    st.error(f\"Alternative clustering failed: {str(e)}\")\n","                    st.write(\"The model and current data features are incompatible.\")\n","                    st.write(\"Consider retraining your model with these features or using a compatible dataset.\")\n","            else:\n","                st.error(\"Cannot find compatible features for this model\")\n","    else:\n","        st.warning(\"Please upload or manually enter data before running clustering analysis.\")\n","\n","if page in [\"Classification\", \"Regression\", \"Clustering\"]:\n","\n","    if st.button(\"Save Predictions\"):\n","        if \"df_features\" in st.session_state and st.session_state.df_features is not None:\n","            file_path = \"predictions.csv\"\n","            st.session_state.df_features.to_csv(file_path, index=False)\n","\n","            st.download_button(\n","                label=\"Download Predictions\",\n","                data=st.session_state.df_features.to_csv(index=False),\n","                file_name=\"predictions.csv\",\n","                mime=\"text/csv\",\n","            )\n","\n","            st.success(\"Predictions saved successfully!\")\n","        else:\n","            st.warning(\"No predictions available to save.\")"],"metadata":{"id":"_qfo6DLdwyQW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753676645717,"user_tz":-330,"elapsed":233,"user":{"displayName":"Gowtham Anbazhagan","userId":"09398642683718980526"}},"outputId":"28c52f83-9ad0-456d-f727-de9176c7c490"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing app.py\n"]}]},{"cell_type":"code","source":["!streamlit run app.py &>/dev/null&"],"metadata":{"id":"z2VJ6rq4Rkw5","executionInfo":{"status":"ok","timestamp":1753676764161,"user_tz":-330,"elapsed":121,"user":{"displayName":"Gowtham Anbazhagan","userId":"09398642683718980526"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from pyngrok import ngrok\n","\n","# Set your authtoken (only needs to be done once per session)\n","ngrok.set_auth_token(\"2xinjqQrzb4Q2nmvw5IT1YdzgTX_7pCJy1kPtLFRq3qh2NbWz\")\n"],"metadata":{"id":"hiYjbOuWUnsQ","executionInfo":{"status":"ok","timestamp":1753677572011,"user_tz":-330,"elapsed":120,"user":{"displayName":"Gowtham Anbazhagan","userId":"09398642683718980526"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["public_url = ngrok.connect(addr=\"8501\", proto=\"http\")\n","\n","print(\"Streamlit app is live at:\", public_url)\n","\n","!streamlit run app.py &>/content/logs.txt &\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ls9d3HHZVLEA","executionInfo":{"status":"ok","timestamp":1753677693947,"user_tz":-330,"elapsed":199,"user":{"displayName":"Gowtham Anbazhagan","userId":"09398642683718980526"}},"outputId":"feca7032-e5e2-467c-b87d-eb9a44cab7d0"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Streamlit app is live at: NgrokTunnel: \"https://97a8aee1c4ce.ngrok-free.app\" -> \"http://localhost:8501\"\n"]}]},{"cell_type":"code","source":["# Kill any existing tunnels\n","ngrok.kill()"],"metadata":{"id":"CUqwrg2bbAKl","executionInfo":{"status":"ok","timestamp":1753679503841,"user_tz":-330,"elapsed":44,"user":{"displayName":"Gowtham Anbazhagan","userId":"09398642683718980526"}}},"execution_count":13,"outputs":[]}]}